{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment is available as a publically available docker container: `hamelsmu/ml-gpu`\n",
    "\n",
    "### Pre-requisite: Familiarize yourself with sequence-to-sequence models\n",
    "\n",
    "If you are not familiar with sequence to sequence models, please refer to [this tutorial](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8).\n",
    "\n",
    "### Pre-Requisite: Make Sure you have the right files prepared from Step 1\n",
    "\n",
    "You should have these files in the root of the `./data/processed_data/` directory:\n",
    "\n",
    "1. `{train/valid/test.function}` - these are python function definitions tokenized (by space), 1 line per function.\n",
    "2. `{train/valid/test.docstring}` - these are docstrings that correspond to each of the python function definitions, and have a 1:1 correspondence with the lines in *.function files.\n",
    "3. `{train/valid/test.lineage}` - every line in this file contains a link back to the original location (github repo link) where the code was retrieved.  There is a 1:1 correspondence with the lines in this file and the other two files. This is useful for debugging.\n",
    "\n",
    "\n",
    "### Set the value of `use_cache` appropriately.  \n",
    "\n",
    "If `use_cache = True`, data will be downloaded where possible instead of re-computing.  However, it is highly recommended that you set `use_cache = False` for this tutorial as it will be less confusing, and you will learn more by runing these steps yourself. **This notebook takes approximately 4 hours to run on an AWS `p3.8xlarge` instance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: you can set what GPU you want to use in a notebook like this.  \n",
    "# # Useful if you want to run concurrent experiments at the same time on different GPUs.\n",
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This will allow the notebook to run faster\n",
    "from pathlib import Path\n",
    "from general_utils import get_step2_prerequisite_files, read_training_files\n",
    "from keras.utils import get_file\n",
    "OUTPUT_PATH = Path('./data/seq2seq/')\n",
    "OUTPUT_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Text From File\n",
    "\n",
    "We want to read in raw text from files so we can pre-process the text for modeling as described in [this tutorial](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Num rows for encoder training + validation input: 1,227,989\n",
      "WARNING:root:Num rows for encoder holdout input: 177,220\n",
      "WARNING:root:Num rows for decoder training + validation input: 1,227,989\n",
      "WARNING:root:Num rows for decoder holdout input: 177,220\n"
     ]
    }
   ],
   "source": [
    "if use_cache:\n",
    "    get_step2_prerequisite_files(output_directory = './data/processed_data')\n",
    "\n",
    "# you want to supply the directory where the files are from step 1.\n",
    "train_code, holdout_code, train_comment, holdout_comment = read_training_files('./data/processed_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code and comment files should be of the same length.\n",
    "\n",
    "assert len(train_code) == len(train_comment)\n",
    "assert len(holdout_code) == len(holdout_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Text\n",
    "\n",
    "In this step, we are going to pre-process the raw text for modeling.  For an explanation of what this section does, see the [Preapre & Clean Data section of this Tutorial](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:Setting maximum document length to 55 based upon hueristic of 0.7 percentile.\n",
      " See full histogram by insepecting the `document_length_stats` attribute.\n",
      "WARNING:root:(1/2) done. 111 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 35 sec\n",
      "WARNING:root:Finished parsing 1,227,989 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 33 sec\n",
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:Setting maximum document length to 15 based upon hueristic of 0.7 percentile.\n",
      " See full histogram by insepecting the `document_length_stats` attribute.\n",
      "WARNING:root:(1/2) done. 38 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 11 sec\n",
      "WARNING:root:Finished parsing 1,227,989 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 13 sec\n"
     ]
    }
   ],
   "source": [
    "from ktext.preprocess import processor\n",
    "\n",
    "if not use_cache:    \n",
    "    code_proc = processor(hueristic_pct_padding=.7, keep_n=20000)\n",
    "    t_code = code_proc.fit_transform(train_code)\n",
    "\n",
    "    comment_proc = processor(append_indicators=True, hueristic_pct_padding=.7, keep_n=14000, padding ='post')\n",
    "    t_comment = comment_proc.fit_transform(train_comment)\n",
    "\n",
    "elif use_cache:\n",
    "    logging.warning('Not fitting transform function because use_cache=True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save tokenized text** (You will reuse this for step 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as dpickle\n",
    "import numpy as np\n",
    "\n",
    "if not use_cache:\n",
    "    # Save the preprocessor\n",
    "    with open(OUTPUT_PATH/'py_code_proc_v2.dpkl', 'wb') as f:\n",
    "        dpickle.dump(code_proc, f)\n",
    "\n",
    "    with open(OUTPUT_PATH/'py_comment_proc_v2.dpkl', 'wb') as f:\n",
    "        dpickle.dump(comment_proc, f)\n",
    "\n",
    "    # Save the processed data\n",
    "    np.save(OUTPUT_PATH/'py_t_code_vecs_v2.npy', t_code)\n",
    "    np.save(OUTPUT_PATH/'py_t_comment_vecs_v2.npy', t_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrange data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder input: (1227989, 55)\n",
      "Shape of decoder input: (1227989, 14)\n",
      "Shape of decoder target: (1227989, 14)\n",
      "Size of vocabulary for data/seq2seq/py_code_proc_v2.dpkl: 20,002\n",
      "Size of vocabulary for data/seq2seq/py_comment_proc_v2.dpkl: 14,002\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor\n",
    "\n",
    "\n",
    "encoder_input_data, encoder_seq_len = load_encoder_inputs(OUTPUT_PATH/'py_t_code_vecs_v2.npy')\n",
    "decoder_input_data, decoder_target_data = load_decoder_inputs(OUTPUT_PATH/'py_t_comment_vecs_v2.npy')\n",
    "num_encoder_tokens, enc_pp = load_text_processor(OUTPUT_PATH/'py_code_proc_v2.dpkl')\n",
    "num_decoder_tokens, dec_pp = load_text_processor(OUTPUT_PATH/'py_comment_proc_v2.dpkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have the above files on disk because you set `use_cache = True` you can download the files for the above function calls here:\n",
    "\n",
    " - https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_t_code_vecs_v2.npy\n",
    " - https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_t_comment_vecs_v2.npy\n",
    " - https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_code_proc_v2.dpkl\n",
    " - https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_comment_proc_v2.dpkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Seq2Seq Model For Summarizing Code\n",
    "\n",
    "We will build a model to predict the docstring given a function or a method.  While this is a very cool task in itself, this is not the end goal of this exercise.  The motivation for training this model is to learn a general purpose feature extractor for code that we can use for the task of code search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_utils import build_seq2seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convenience function `build_seq2seq_model` constructs the architecture for a sequence-to-sequence model.  \n",
    "\n",
    "The architecture built for this tutorial is a minimal example with only one layer for the encoder and decoder, and does not include things like [attention](https://nlp.stanford.edu/pubs/emnlp15_attn.pdf).  We encourage you to try and build different architectures to see what works best for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_Model = build_seq2seq_model(word_emb_dim=800,\n",
    "                                    hidden_state_dim=1000,\n",
    "                                    encoder_seq_len=encoder_seq_len,\n",
    "                                    num_encoder_tokens=num_encoder_tokens,\n",
    "                                    num_decoder_tokens=num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Word-Embedding (Embeddi (None, None, 800)    11201600    Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      (None, 55)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-1 (BatchNorma (None, None, 800)    3200        Decoder-Word-Embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Model (Model)           (None, 1000)         21407800    Encoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-GRU (GRU)               [(None, None, 1000), 5403000     Decoder-Batchnorm-1[0][0]        \n",
      "                                                                 Encoder-Model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-2 (BatchNorma (None, None, 1000)   4000        Decoder-GRU[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Final-Output-Dense (Dense)      (None, None, 14002)  14016002    Decoder-Batchnorm-2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 52,035,602\n",
      "Trainable params: 52,030,402\n",
      "Non-trainable params: 5,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq2seq_Model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1080630 samples, validate on 147359 samples\n",
      "Epoch 1/16\n",
      "1080630/1080630 [==============================] - 527s 487us/step - loss: 4.0323 - val_loss: 3.4703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/engine/topology.py:2379: UserWarning: Layer Decoder-GRU was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Model/Encoder-Last-GRU/while/Exit_2:0' shape=(?, 1000) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/16\n",
      "1080630/1080630 [==============================] - 520s 481us/step - loss: 3.2246 - val_loss: 3.1348\n",
      "Epoch 3/16\n",
      "1080630/1080630 [==============================] - 520s 481us/step - loss: 2.9401 - val_loss: 2.9538\n",
      "Epoch 4/16\n",
      "1080630/1080630 [==============================] - 520s 481us/step - loss: 2.7674 - val_loss: 2.8410\n",
      "Epoch 5/16\n",
      "1080630/1080630 [==============================] - 520s 481us/step - loss: 2.6459 - val_loss: 2.7659\n",
      "Epoch 6/16\n",
      "1080630/1080630 [==============================] - 521s 482us/step - loss: 2.5528 - val_loss: 2.7149\n",
      "Epoch 7/16\n",
      "1080630/1080630 [==============================] - 521s 482us/step - loss: 2.4766 - val_loss: 2.6760\n",
      "Epoch 8/16\n",
      "1080630/1080630 [==============================] - 521s 482us/step - loss: 2.4130 - val_loss: 2.6432\n",
      "Epoch 9/16\n",
      "1080630/1080630 [==============================] - 521s 482us/step - loss: 2.3581 - val_loss: 2.6233\n",
      "Epoch 10/16\n",
      "1080630/1080630 [==============================] - 521s 482us/step - loss: 2.3097 - val_loss: 2.6049\n",
      "Epoch 11/16\n",
      "1080630/1080630 [==============================] - 521s 482us/step - loss: 2.2668 - val_loss: 2.5909\n",
      "Epoch 12/16\n",
      "1080630/1080630 [==============================] - 520s 482us/step - loss: 2.2280 - val_loss: 2.5778\n",
      "Epoch 13/16\n",
      "1080630/1080630 [==============================] - 520s 482us/step - loss: 2.1928 - val_loss: 2.5684\n",
      "Epoch 14/16\n",
      "1080630/1080630 [==============================] - 521s 482us/step - loss: 2.1601 - val_loss: 2.5596\n",
      "Epoch 15/16\n",
      "1080630/1080630 [==============================] - 521s 482us/step - loss: 2.1300 - val_loss: 2.5558\n",
      "Epoch 16/16\n",
      "1080630/1080630 [==============================] - 521s 482us/step - loss: 2.1021 - val_loss: 2.5528\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, load_model\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "if not use_cache:\n",
    "\n",
    "    from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "    import numpy as np\n",
    "    from keras import optimizers\n",
    "\n",
    "    seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.00005), loss='sparse_categorical_crossentropy')\n",
    "\n",
    "    script_name_base = 'py_func_sum_v9_'\n",
    "    csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n",
    "                                       save_best_only=True)\n",
    "\n",
    "    batch_size = 1100\n",
    "    epochs = 16\n",
    "    history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split=0.12, callbacks=[csv_logger, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cache:\n",
    "    logging.warning('Not re-training function summarizer seq2seq model because use_cache=True')\n",
    "    # Load model from url\n",
    "    loc = get_file(fname='py_func_sum_v9_.epoch16-val2.55276.hdf5',\n",
    "                   origin='https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_func_sum_v9_.epoch16-val2.55276.hdf5')\n",
    "    seq2seq_Model = load_model(loc)\n",
    "    \n",
    "    # Load encoder (code) pre-processor from url\n",
    "    loc = get_file(fname='py_code_proc_v2.dpkl',\n",
    "                   origin='https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_code_proc_v2.dpkl')\n",
    "    num_encoder_tokens, enc_pp = load_text_processor(loc)\n",
    "    \n",
    "    # Load decoder (docstrings/comments) pre-processor from url\n",
    "    loc = get_file(fname='py_comment_proc_v2.dpkl',\n",
    "                   origin='https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_comment_proc_v2.dpkl')\n",
    "    num_decoder_tokens, dec_pp = load_text_processor(loc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above procedure will automatically download a pre-trained model and associated artifacts from https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/ if `use_cache = True`.  \n",
    "\n",
    "Otherwise, the above code will checkpoint the best model after each epoch into the current directory with prefix `py_func_sum_v9_`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Seq2Seq Model For Code Summarization\n",
    "\n",
    "To evaluate this model we are going to do two things:\n",
    "\n",
    "1.  Manually inspect the results of predicted docstrings for code snippets, to make sure they look sensible.\n",
    "2.  Calculate the [BLEU Score](https://en.wikipedia.org/wiki/BLEU) so that we can quantitately benchmark different iterations of this algorithm and to guide hyper-parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Inspect Results (on holdout set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 44697 =================\n",
      "\n",
      "Original Input:\n",
      " def testInitialization self event_formatter task_scheduler TaskCacheEventFormatter self assertIsNotNone event_formatter\n",
      " \n",
      "\n",
      "Original Output:\n",
      " tests the initialization .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " tests the initialization of the event\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 151630 =================\n",
      "\n",
      "Original Input:\n",
      " def reduceChat self cmd text attrs text cmd script_parser parse_text text val cmd script_parser _ Chat message channel get_attr cmd chat attrs channel cls get_attr cmd chat attrs cls validate_attrs cmd chat attrs text channel cls args if channel is not None args channel channel if cls is not None args cls cls self val chat text args\n",
      " \n",
      "\n",
      "Original Output:\n",
      " % reduce chat scalar exprattrs\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " parse command line arguments and return the result\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 67584 =================\n",
      "\n",
      "Original Input:\n",
      " def tabs self length tab_width thickness direction 0 args tab_width max 1 5 thickness tab_width nb_tabs math floor length tab_width nb_tabs int nb_tabs 1 nb_tabs 2 tab_real_width length nb_tabs if tab_real_width thickness 1 5 raise BoxGenrationError Attention les encoches resultantes 2f mm ne sont pas assez larges au vue de l epasseur de votre materiaux Merci d augmenter la largeur des encoches tab_real_width return self _rotate_path self _generate_tabs_path tab_real_width nb_tabs thickness direction direction args direction\n",
      " \n",
      "\n",
      "Original Output:\n",
      " \"* genere les elements d'un polygone * svg pour des encoche d'approximativement * < tab_width > , sur un longueur de < length > , * pour un materiau d'epaisseur < thickness>. * * options : * - direction : 0 haut de la face , 1 droite de la face , 2 bas de la face , 3 gauche de la face . * - firstup : indique si l'on demarre en haut d'un crenau ( true ) ou en bas du crenau ( false - defaut ) * - lastup : indique si l'on fin en haut d'un crenau ( true ) ou en bas du crenau ( false - defaut )\"\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " returns a list of tabs\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 54474 =================\n",
      "\n",
      "Original Input:\n",
      " def copy_script self original_script new_key None new_obj None new_locks None typeclass original_script typeclass_path new_key new_key if new_key is not None else original_script key new_obj new_obj if new_obj is not None else original_script obj new_locks new_locks if new_locks is not None else original_script db_lock_storage from evennia utils import create new_script create create_script typeclass key new_key obj new_obj locks new_locks autostart True return new_script\n",
      " \n",
      "\n",
      "Original Output:\n",
      " make an identical copy of the original_script .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " copies a script from the given script to the given path\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 38202 =================\n",
      "\n",
      "Original Input:\n",
      " def grouper n iterable fillvalue None args iter iterable n return izip_longest args fillvalue fillvalue\n",
      " \n",
      "\n",
      "Original Output:\n",
      " \"grouper(3 , ' abcdefg ' , ' x ' ) -- > abc def gxx\"\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " grouper number x abc def abc def\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 159500 =================\n",
      "\n",
      "Original Input:\n",
      " property def file_length self return self _file_length\n",
      " \n",
      "\n",
      "Original Output:\n",
      " \"length of the input file , in bytes .\"\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " the length of the file\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 25249 =================\n",
      "\n",
      "Original Input:\n",
      " property def ptr self if self _ptr return self _ptr else raise GEOSException NULL coordinate sequence pointer encountered\n",
      " \n",
      "\n",
      "Original Output:\n",
      " \"property for controlling access to coordinate sequence pointer , preventing attempted access to a null memory location .\"\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " return the pointer to the ptr record\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 57374 =================\n",
      "\n",
      "Original Input:\n",
      " def _subscribed self self logger debug Subscription confirmed self state subscribed for message in self message_queue self send message\n",
      " \n",
      "\n",
      "Original Output:\n",
      " called when the subscription was accepted successfully .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " sends a subscription to the subscribed subscription\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 30996 =================\n",
      "\n",
      "Original Input:\n",
      " def tearDown self del self user\n",
      " \n",
      "\n",
      "Original Output:\n",
      " clean up the user instance after each test method .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " clean up after each test\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 35211 =================\n",
      "\n",
      "Original Input:\n",
      " staticmethod def execute_script sql_statements commit True cursor db cursor cursor executescript sql_statements if commit db commit return cursor\n",
      " \n",
      "\n",
      "Original Output:\n",
      " \"execute a script of statements , and optionally commit . @param sql_statements : a string containing multiple sql statements , separated by ' ; ' @param commit : if true , autocommit after executing the statements ( default : true ) @return : a sqlite3 cursor object .\"\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " executes sql statements and returns the result\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 44389 =================\n",
      "\n",
      "Original Input:\n",
      " def run_in_twisted self url DEFAULT_AUTOBAHN_ROUTER realm DEFAULT_AUTOBAHN_REALM authmethods None authid None authrole None authextra None callback None kwargs _init_crochet in_twisted True logger debug run_in_crossbar bootstraping blocking callback is None def bootstrap_and_callback self _bootstrap blocking url url realm realm authmethods authmethods authid authid authrole authrole authextra authextra kwargs if callback callback self _callbacks_runner start threads deferToThread bootstrap_and_callback\n",
      " \n",
      "\n",
      "Original Output:\n",
      " \"start the wamp connection . given we can not run synchronous stuff inside the twisted thread , use this function ( which returns immediately ) to do the initialization from a spawned thread .\"\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " run a twisted web server\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 133401 =================\n",
      "\n",
      "Original Input:\n",
      " abc abstractmethod def create_image self container_format image_location disk_format\n",
      " \n",
      "\n",
      "Original Output:\n",
      " creates new image .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " creates a container image\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 133525 =================\n",
      "\n",
      "Original Input:\n",
      " abc abstractmethod def create_snapshot self volume_id\n",
      " \n",
      "\n",
      "Original Output:\n",
      " creates a volume snapshot .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " create a snapshot\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 100859 =================\n",
      "\n",
      "Original Input:\n",
      " def GetNextWaitTime self raise NotImplementedError GetNextWaitTime not implemented\n",
      " \n",
      "\n",
      "Original Output:\n",
      " suggest time to wait before issuing next checksum query .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " returns the current state of the device\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 95136 =================\n",
      "\n",
      "Original Input:\n",
      " def subtract_from_now td now datetime now return int now td strftime s 1000\n",
      " \n",
      "\n",
      "Original Output:\n",
      " subtract timedelta from current time\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " subtract the current time from the current time\n"
     ]
    }
   ],
   "source": [
    "from seq2seq_utils import Seq2Seq_Inference\n",
    "import pandas as pd\n",
    "\n",
    "seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=enc_pp,\n",
    "                                 decoder_preprocessor=dec_pp,\n",
    "                                 seq2seq_model=seq2seq_Model)\n",
    "\n",
    "demo_testdf = pd.DataFrame({'code':holdout_code, 'comment':holdout_comment, 'ref':''})\n",
    "seq2seq_inf.demo_model_predictions(n=15, df=demo_testdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on manual inspection of results:\n",
    "\n",
    "The predicted code summaries are not perfect, but we can see that the model has learned to extract some semantic meaning from the code.  That's all we need to get reasonable results in this case.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate BLEU Score (on holdout set)\n",
    "\n",
    "BLEU Score is described [in this wikipedia article](https://en.wikipedia.org/wiki/BLEU), and is a way to measure the efficacy of summarization/translation such as the one we conducted here.  This metric is useful if you wish to conduct extensive hyper-parameter tuning and try to improve the seq2seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Generating predictions.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e0663e88db4cdfb90e7680aef26329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=177220), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Calculating BLEU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07134319200840655"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will return a BLEU Score\n",
    "seq2seq_inf.evaluate_model(input_strings=holdout_code, \n",
    "                           output_strings=holdout_comment, \n",
    "                           max_len=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model to disk\n",
    "\n",
    "Save the model to disk so you can use it in Step 4 of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/engine/topology.py:2379: UserWarning: Layer Decoder-GRU was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Model/Encoder-Last-GRU/while/Exit_2:0' shape=(?, 1000) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "seq2seq_Model.save(OUTPUT_PATH/'code_summary_seq2seq_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
